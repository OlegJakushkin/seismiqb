{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative research\n",
    "\n",
    "In this advanced notebook, we apply the [carcass interpolation model](./01_Demo_E.ipynb), as well as [horizon extension](./../Horizon_extension/Demo_E.ipynb) and enhancement ones in a quick succesion with the help of [research](./../Research_template.ipynb). It is adviced to check out our notebooks on these techniques prior to looking at this one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "from copy import copy\n",
    "from glob import glob\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import date\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('../..')\n",
    "from seismiqb.batchflow import Pipeline, Dataset, C, V, R, P, B, D\n",
    "from seismiqb.batchflow.models.torch import EncoderDecoder, ResBlock\n",
    "from seismiqb.batchflow.research import Research, Option, Domain, Results, FileLogger\n",
    "from seismiqb.batchflow.research import RP, RC, KV\n",
    "\n",
    "from seismiqb import Interpolator, Enhancer, Extender\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global parameters\n",
    "DEVICES = [0, 1, 2, 3, 4, 5, 6, 7]         # physical device numbers\n",
    "WORKERS = len(DEVICES)\n",
    "\n",
    "RESEARCH_NAME = f'Research_iterative'\n",
    "DUMP_NAME = date.today().strftime(\"%Y-%d-%m\") + RESEARCH_NAME[8:]\n",
    "N_REPS = 2\n",
    "SUPPORTS = 100\n",
    "OVERLAP_FACTOR = 2.\n",
    "ITERATIONS = 1\n",
    "FREQUENCIES = [200, 200]\n",
    "\n",
    "\n",
    "# Detection parameters\n",
    "DETECTION_CROP_SHAPE = (1, 256, 256)       # shape of sampled 3D crops\n",
    "DETECTION_ITERS = 500                      # number of train iterations\n",
    "DETECTION_BATCH_SIZE = 64                  # number of crops inside one batch\n",
    "\n",
    "\n",
    "# Extension parameters\n",
    "EXTENSION_CROP_SHAPE = (1, 64, 64)         # shape of sampled 3D crops\n",
    "EXTENSION_ITERS = 500                      # number of train iterations\n",
    "EXTENSION_BATCH_SIZE = 64                  # number of crops inside one batch\n",
    "EXTENSION_STRIDE = 32                      # step size for extension\n",
    "EXTENSION_STEPS = 50                       # number of boundary extensions\n",
    "\n",
    "\n",
    "# Enhancing parameters\n",
    "ENHANCE_CROP_SHAPE = (1, 256, 256)         # shape of sampled 3D crops\n",
    "ENHANCE_ITERS = 500                        # number of train iterations\n",
    "ENHANCE_BATCH_SIZE = 64                    # number of crops inside one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "DETECTION_MODEL_CONFIG = {\n",
    "    # Model layout\n",
    "    'initial_block': {\n",
    "        'base_block': ResBlock,\n",
    "        'filters': 16,\n",
    "        'kernel_size': 5,\n",
    "        'downsample': False,\n",
    "        'attention': 'scse'\n",
    "    },\n",
    "\n",
    "    'body/encoder': {\n",
    "        'num_stages': 4,\n",
    "        'order': 'sbd',\n",
    "        'blocks': {\n",
    "            'base': ResBlock,\n",
    "            'n_reps': 1,\n",
    "            'filters': [32, 64, 128, 256],\n",
    "            'attention': 'scse',\n",
    "        },\n",
    "    },\n",
    "    'body/embedding': {\n",
    "        'base': ResBlock,\n",
    "        'n_reps': 1,\n",
    "        'filters': 256,\n",
    "        'attention': 'scse',\n",
    "    },\n",
    "    'body/decoder': {\n",
    "        'num_stages': 4,\n",
    "        'upsample': {\n",
    "            'layout': 'tna',\n",
    "            'kernel_size': 2,\n",
    "        },\n",
    "        'blocks': {\n",
    "            'base': ResBlock,\n",
    "            'filters': [128, 64, 32, 16],\n",
    "            'attention': 'scse',\n",
    "        },\n",
    "    },\n",
    "    'head': {\n",
    "        'base_block': ResBlock,\n",
    "        'filters': [16, 8],\n",
    "        'attention': 'scse'\n",
    "    },\n",
    "    'output': 'sigmoid',\n",
    "    # Train configuration\n",
    "    'loss': 'bdice',\n",
    "    'optimizer': {'name': 'Adam', 'lr': 0.01,},\n",
    "    'decay': {'name': 'exp', 'gamma': 0.1, 'frequency': 150},\n",
    "    'microbatch': 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "EXTENSION_MODEL_CONFIG = {\n",
    "    # Model layout\n",
    "    'body/encoder': {\n",
    "        'num_stages': 4,\n",
    "        'order': 'sbd',\n",
    "        'blocks': {\n",
    "            'base': ResBlock,\n",
    "            'n_reps': 1,\n",
    "            'filters': [32, 64, 128, 256],\n",
    "            'attention': 'scse',\n",
    "        },\n",
    "    },\n",
    "    'body/embedding': {\n",
    "        'base': ResBlock,\n",
    "        'n_reps': 1,\n",
    "        'filters': 256,\n",
    "        'attention': 'scse',\n",
    "    },\n",
    "    'body/decoder': {\n",
    "        'num_stages': 4,\n",
    "        'upsample': {\n",
    "            'layout': 'tna',\n",
    "            'kernel_size': 2,\n",
    "        },\n",
    "        'blocks': {\n",
    "            'base': ResBlock,\n",
    "            'filters': [128, 64, 32, 16],\n",
    "            'attention': 'scse',\n",
    "        },\n",
    "    },\n",
    "    'head': {\n",
    "        'base_block': ResBlock,\n",
    "        'filters': [16, 8],\n",
    "        'attention': 'scse'\n",
    "    },\n",
    "    'output': 'sigmoid',\n",
    "    # Train configuration\n",
    "    'loss': 'bdice',\n",
    "    'optimizer': {'name': 'Adam', 'lr': 0.005,},\n",
    "    'decay': {'name': 'exp', 'gamma': 0.1, 'frequency': 150},\n",
    "    'microbatch': 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ENHANCE_MODEL_CONFIG = {\n",
    "    # Model layout\n",
    "    'body/encoder': {\n",
    "        'num_stages': 4,\n",
    "        'order': 'sbd',\n",
    "        'blocks': {\n",
    "            'base': ResBlock,\n",
    "            'n_reps': 1,\n",
    "            'filters': [32, 64, 128, 256],\n",
    "            'attention': 'scse',\n",
    "        },\n",
    "    },\n",
    "    'body/embedding': {\n",
    "        'base': ResBlock,\n",
    "        'n_reps': 1,\n",
    "        'filters': 256,\n",
    "        'attention': 'scse',\n",
    "    },\n",
    "    'body/decoder': {\n",
    "        'num_stages': 4,\n",
    "        'upsample': {\n",
    "            'layout': 'tna',\n",
    "            'kernel_size': 2,\n",
    "        },\n",
    "        'blocks': {\n",
    "            'base': ResBlock,\n",
    "            'filters': [128, 64, 32, 16],\n",
    "            'attention': 'scse',\n",
    "        },\n",
    "    },\n",
    "    'head': {\n",
    "        'base_block': ResBlock,\n",
    "        'filters': [16, 8],\n",
    "        'attention': 'scse'\n",
    "    },\n",
    "    'output': 'sigmoid',\n",
    "    # Train configuration\n",
    "    'loss': 'bdice',\n",
    "    'optimizer': {'name': 'Adam', 'lr': 0.01,},\n",
    "    'decay': {'name': 'exp', 'gamma': 0.1, 'frequency': 150},\n",
    "    'microbatch': 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\n",
    "    ('/data/seismic/CUBE_2/M_cube.hdf5', '/data/seismic/CUBE_2/RAW/*'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrolled = [\n",
    "    (cube_path, horizon_path)\n",
    "    for cube_path, horizon_dir in paths\n",
    "    for horizon_path in glob(horizon_dir)\n",
    "]\n",
    "\n",
    "options = [\n",
    "    KV((cube_path, horizon_path),\n",
    "       '+'.join((cube_path.split('/')[-1].split('.')[0], horizon_path.split('/')[-1].split('.')[0])))\n",
    "    for cube_path, horizon_path in unrolled\n",
    "]\n",
    "random.shuffle(options)\n",
    "\n",
    "domain = Option('cube_and_horizon', options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_one_experiment(config, ppl):\n",
    "    \n",
    "    ###################################################################################\n",
    "    ################################   PARSE CONFIGS   ################################\n",
    "    ###################################################################################\n",
    "    # Get all the params from configs\n",
    "    config = config.config()\n",
    "    train_cube, horizon = config['cube_and_horizon']\n",
    "    n_rep = config['repetition']\n",
    "    \n",
    "    \n",
    "    # Directory to save results to\n",
    "    results_dir = os.path.join(RESEARCH_NAME, 'custom_results')\n",
    "    \n",
    "    short_name_cube = train_cube.split('/')[-1].split('.')[0]\n",
    "    short_name_horizon = horizon.split('/')[-1].split('.')[0]\n",
    "    alias = os.path.join(short_name_cube, short_name_horizon, f'{n_rep}')\n",
    "    save_dir = os.path.join(results_dir, alias)\n",
    "    \n",
    "    return_value = [[], [], [], []] # coverages, window ratios, support corrs, local corrs\n",
    "    \n",
    "\n",
    "    ###################################################################################\n",
    "    ##################################   DETECTION   ##################################\n",
    "    ###################################################################################\n",
    "    # Create Detector instance\n",
    "    detector = Interpolator(\n",
    "        batch_size=DETECTION_BATCH_SIZE,\n",
    "        crop_shape=DETECTION_CROP_SHAPE,\n",
    "        model_config=DETECTION_MODEL_CONFIG,\n",
    "        save_dir=save_dir, bar=False\n",
    "    )\n",
    "    \n",
    "    train_dataset = detector.make_dataset(train_cube,\n",
    "                                          {short_name_cube : [horizon]})\n",
    "\n",
    "    # Train model\n",
    "    last_loss = detector.train(dataset=train_dataset,\n",
    "                               frequencies=FREQUENCIES,\n",
    "                               n_iters=DETECTION_ITERS,\n",
    "                               width=5, batch_size_multiplier=1,\n",
    "                               rebatch_threshold=0.9)\n",
    "    \n",
    "\n",
    "    # Inference on the same cube to interpolate horizon on whole spatial range\n",
    "    detector.inference(dataset=train_dataset,\n",
    "                       batch_size_multiplier=0.1,\n",
    "                       version=1, orientation='ix',\n",
    "                       overlap_factor=OVERLAP_FACTOR)\n",
    "\n",
    "    infos = detector.evaluate(n=1, add_prefix=False, dump=True, supports=SUPPORTS)\n",
    "    info = infos[0]\n",
    "    horizon = detector.predictions[0]\n",
    "    \n",
    "    return_value[0].append(horizon.coverage)\n",
    "    return_value[1].append(info['window_rate'])\n",
    "    return_value[2].append(info['corrs'])\n",
    "    return_value[3].append(info['local_corrs'])\n",
    "\n",
    "    \n",
    "    for i in range(ITERATIONS):\n",
    "        ###################################################################################\n",
    "        ###################################   EXTEND   ####################################\n",
    "        ###################################################################################\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Create instance of Enhancer\n",
    "        extender = Extender(\n",
    "            batch_size=EXTENSION_BATCH_SIZE,\n",
    "            crop_shape=EXTENSION_CROP_SHAPE,\n",
    "            model_config=EXTENSION_MODEL_CONFIG,\n",
    "            save_dir=os.path.join(save_dir, f'extended_{i}'), bar=False\n",
    "        )\n",
    "\n",
    "        # Train model\n",
    "        extender.train(horizon, n_iters=EXTENSION_ITERS, width=5)\n",
    "\n",
    "        # Inference: fill the holes and exterior\n",
    "        horizon = extender.inference(horizon,\n",
    "                                     n_steps=EXTENSION_STEPS,\n",
    "                                     stride=EXTENSION_STRIDE)\n",
    "\n",
    "        # Evaluate results\n",
    "        horizon = extender.predictions[0]\n",
    "        extender.targets = detector.targets\n",
    "        infos = extender.evaluate(n=1, add_prefix=False, dump=True, supports=SUPPORTS)\n",
    "        info = infos[0]\n",
    "        \n",
    "        return_value[0].append(horizon.coverage)\n",
    "        return_value[1].append(info['window_rate'])\n",
    "        return_value[2].append(info['corrs'])\n",
    "        return_value[3].append(info['local_corrs'])\n",
    "\n",
    "\n",
    "        ###################################################################################\n",
    "        ###################################   ENHANCE   ###################################\n",
    "        ###################################################################################\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Create instance of Enhancer\n",
    "        enhancer = Enhancer(\n",
    "            batch_size=ENHANCE_BATCH_SIZE,\n",
    "            crop_shape=ENHANCE_CROP_SHAPE,\n",
    "            model_config=ENHANCE_MODEL_CONFIG,\n",
    "            save_dir=os.path.join(save_dir, f'enhanced_{i}'), bar=False\n",
    "        )\n",
    "\n",
    "        # Train model\n",
    "        enhancer.train(horizon, n_iters=ENHANCE_ITERS, width=5)\n",
    "\n",
    "        # Inference: try to make every crop a touch better\n",
    "        enhancer.inference(horizon,\n",
    "                           batch_size_multiplier=0.1,\n",
    "                           version=1, orientation='ix',\n",
    "                           overlap_factor=OVERLAP_FACTOR)\n",
    "\n",
    "        # Evaluate results\n",
    "        enhancer.targets = detector.targets\n",
    "        infos = enhancer.evaluate(n=1, add_prefix=False, dump=True, supports=SUPPORTS)\n",
    "        info = infos[0]\n",
    "        horizon = enhancer.predictions[0]\n",
    "    \n",
    "        return_value[0].append(horizon.coverage)\n",
    "        return_value[1].append(info['window_rate'])\n",
    "        return_value[2].append(info['corrs'])\n",
    "        return_value[3].append(info['local_corrs'])\n",
    "\n",
    "    ###################################################################################\n",
    "    ##############################   SAVE NEXT TO CUBE   ##############################\n",
    "    ###################################################################################\n",
    "    cube_dir = os.path.dirname(horizon.geometry.path)\n",
    "    savepath = os.path.join(cube_dir, 'HORIZONS_DUMP', DUMP_NAME)\n",
    "    os.makedirs(savepath, exist_ok=True)\n",
    "    horizon.name = '+' + horizon.name.replace('enhanced_', '').replace('extended_', '')\n",
    "    savepath = os.path.join(savepath, horizon.name)\n",
    "    horizon.dump(savepath, add_height=False)\n",
    "    detector.log(f'Dumped horizon to {savepath}')\n",
    "\n",
    "    ###################################################################################\n",
    "    ###################################   RETURNS   ###################################\n",
    "    ###################################################################################\n",
    "    \n",
    "    msg = ''\n",
    "    for name, value in zip(returned_values, return_value):\n",
    "        msg += f'        {name} -> {value}\\n'\n",
    "    detector.log(msg)\n",
    "    return return_value\n",
    "\n",
    "\n",
    "\n",
    "def clear_previous_results(res_name):\n",
    "    if os.path.exists(res_name):\n",
    "        shutil.rmtree(res_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the directory to save logs and results in\n",
    "\n",
    "clear_previous_results(RESEARCH_NAME)\n",
    "\n",
    "returned_values = [\n",
    "    'coverages', 'window_rates', 'corrs', 'local_corrs',\n",
    "]\n",
    "\n",
    "# Fake pipeline is needed to pass parameters around\n",
    "fake_ppl = Pipeline().set_dataset(Dataset(10)).run_later(1, n_iters=1)\n",
    "\n",
    "research = (\n",
    "    Research()\n",
    "    .add_logger(FileLogger)\n",
    "    .init_domain(domain, n_reps=N_REPS)\n",
    "    .add_pipeline(fake_ppl, run=True, name='fake')\n",
    "    .add_callable(\n",
    "        perform_one_experiment,                         # Callable to run\n",
    "        returns=returned_values,                        # Names of returned results\n",
    "        execute='#0',                                   # Execute immediately\n",
    "        config=RC('fake'),                              # Pass config to the callable\n",
    "        ppl=RP('fake'),                                 # Pass pipeline to the callable\n",
    "        name='perform_one_experiment'                   # Name to be shown in the dataframe\n",
    "    )\n",
    ")\n",
    "\n",
    "research.run(\n",
    "    n_iters=1,\n",
    "    name=RESEARCH_NAME,\n",
    "    bar=True,\n",
    "    workers=WORKERS,\n",
    "    devices=DEVICES,\n",
    "    timeout=10000,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
